{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Primeros pasos con BeautifulSoup para extracción de datos\n",
        "\n",
        "1.- Importamos la librería"
      ],
      "metadata": {
        "id": "tAbbopClF8tZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import urllib.request"
      ],
      "metadata": {
        "id": "aSFvhvrQ3Qt0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.- Elegimos una dirección en Internet (url).\n",
        "\n",
        "La librería utiliza la función request y urlopen para \"abrir\" la página y \"descargar\" su contenido, el cual queda alojado o almacenado en redditFile.\n",
        "\n",
        "La fuinción read() lee el contenido de redditFile para interpretarlo como html (página web)"
      ],
      "metadata": {
        "id": "p7saqMqJGM53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "redditFile = urllib.request.urlopen('https://www.reddit.com/r/mexico/')\n",
        "redditHtml = redditFile.read()\n",
        "redditFile.close()"
      ],
      "metadata": {
        "id": "WIlD8D4y3dkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.- Enviar el contenido de redditHtml a la librería BeautifulSoup\n",
        "\n",
        "La función BeautifulSoup() interpreta el contenido de redditHtml y la función find_all() busca aquellas etiquetas html que sean enlaces, por ejemplo:\n",
        "\n",
        "```\n",
        "<a href=\"https://NombreDelSitio\">Enlace</a>\n",
        "```\n",
        "El for lee cada uno de los enlaces en búsqueda de la etiqueta href y los imprime"
      ],
      "metadata": {
        "id": "KStwcAmtHSCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(redditHtml)\n",
        "redittAll = soup.find_all('a')\n",
        "for links in redittAll:\n",
        "  print(links.get('href'))"
      ],
      "metadata": {
        "id": "28Xo5LlV-o9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos observar que los enlaces de interés tienen la etiqueta 'comments' por ejemplo:\n",
        "\n",
        "```\n",
        "/r/mexico/comments/16vfdls/historia_familiar_perversa_no_debí_haber/\n",
        "```\n",
        "\n",
        "4.- Hacer un filtro para conservar solo los enlaces que contengan la palabra 'comments'"
      ],
      "metadata": {
        "id": "89cdON71I0hY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(redditHtml)\n",
        "redittAll = soup.find_all('a')\n",
        "for links in redittAll:\n",
        "  if 'comments' in links.get('href'):\n",
        "    print(links.get('href'))"
      ],
      "metadata": {
        "id": "kvF0-Oo3_Fnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.- Extraer texto de la página."
      ],
      "metadata": {
        "id": "eLyHcNIHKfpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(redditHtml)\n",
        "redditAll = soup.find_all('p')\n",
        "for etiqueta in redditAll:\n",
        "  print(etiqueta)"
      ],
      "metadata": {
        "id": "WjHoxcCAJ7Oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo con noticias del periódico El Universal"
      ],
      "metadata": {
        "id": "ZQwPpI6tJv4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "UniversalFile = urllib.request.urlopen('https://www.eluniversal.com.mx/')\n",
        "UniversalHtml = UniversalFile.read()\n",
        "UniversalFile.close()"
      ],
      "metadata": {
        "id": "o-tdRr8cgt4f"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(UniversalHtml)\n",
        "UniversalAll = soup.find_all('a')\n",
        "for links in UniversalAll:\n",
        "  print(links.get('href'))"
      ],
      "metadata": {
        "id": "TZwWUvVyg2p-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(UniversalHtml)\n",
        "UniversalAll = soup.find_all('a')\n",
        "for links in UniversalAll:\n",
        "  if 'tendencias' in links.get('href'):\n",
        "    print(links.get('href'))"
      ],
      "metadata": {
        "id": "edI2WCnLhe5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nota = 'https://www.eluniversal.com.mx//tendencias/fallece-mario-tascon-pionero-del-periodismo-digital/'\n",
        "UniversalFile = urllib.request.urlopen(nota)\n",
        "UniversalHtml = UniversalFile.read()\n",
        "UniversalFile.close()"
      ],
      "metadata": {
        "id": "m3NC0XbYh-K_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(UniversalHtml)\n",
        "UniversalAll = soup.find_all('p')\n",
        "for etiqueta in UniversalAll:\n",
        "  print(etiqueta)"
      ],
      "metadata": {
        "id": "4fxWLf7SjzyZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}